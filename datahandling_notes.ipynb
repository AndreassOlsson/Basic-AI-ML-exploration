{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreassOlsson/Basic-AI-ML-exploration/blob/main/datahandling_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT3AtU1hIZQ3"
      },
      "source": [
        "# Uploading, streaming and sampling data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ79Bc2GIZQ8"
      },
      "source": [
        "### Uploading small amounts of data into memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W5Rt7fvyIZQ9"
      },
      "outputs": [],
      "source": [
        "with open('file_name.txt', 'r') as open_file: # 'r' represents read mode\n",
        "    print('file_name.txt content:\\n' + open_file.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjUkg2q9IZRA"
      },
      "source": [
        "### Uploadning large amounts of data into memory"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "ZnA6W-m-IZRB"
      },
      "source": [
        "Some datasets are too large for an import directly into memory, therefor you can select samples of the dataset to preview. One way to achieve that, is with the \"for, in\" function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "svUI3r4JIZRC",
        "outputId": "5e27e8e6-0dba-4885-d2e0-6886c7cdc474"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'file_name.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-83-a94b39e4bd4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'file_name.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mobservation\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading Data: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'file_name.txt'"
          ]
        }
      ],
      "source": [
        "with open('file_name.txt', 'r') as open_file: \n",
        "    for observation in open_file:\n",
        "        print('Reading Data: ' + observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxE4mWX1IZRC"
      },
      "source": [
        "### Sampling image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0g6mJQ-IZRD"
      },
      "outputs": [],
      "source": [
        "import matplotlib.image as img\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "image = img.imread(\"file_name.jpg\")\n",
        "print(image.shape) # (Horizontal, vertical, depth)\n",
        "print(image.size) # Product of image.shape\n",
        "plt.imshow(image)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDoIpd4QIZRE"
      },
      "source": [
        "### Sampling data by row number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGJpuVbIZRF"
      },
      "source": [
        "Data streaming obtains all the records from a data source. If you don´t need all the records, save time by simply sampling the data, for example by only viewing every fifth item."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPyqHUGuIZRW"
      },
      "outputs": [],
      "source": [
        "n = 2 # View every second item\n",
        "with open('file_name.txt', 'r') as open_file: \n",
        "    for j, observation in enumerate(open_file): \n",
        "        if j % n==0: \n",
        "            print('Reading Line: ' + str(j) + \n",
        "                 ' Content:' + observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aOSSofLIZRX"
      },
      "source": [
        "### Random sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhojTJALIZRY"
      },
      "outputs": [],
      "source": [
        "from random import random\n",
        "sample_size = 0.25 # Will display 25% of information\n",
        "with open('file_name.txt', 'r') as open_file:\n",
        "    for j, observation in enumerate(open_file):\n",
        "        if random()<=sample_size:\n",
        "            print('Reading Line: ' + str(j) + \n",
        "                 ' Content:' + observation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzDfyNSYIZRY"
      },
      "source": [
        "# Data input isn´t intelligent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpc2Up36IZRZ"
      },
      "source": [
        "A problem with using native python techniques is that the input isn´t intelligent. For example, python reads a header as yet more data to process and not a header. You can´t easily select a particular column of data. The Pandas library used in the sections that follow makes it much easier to read and understand flat-file data. Classes and methods in the Pandas library interpret (parse) the flat-tile data to make it easier to manipulate.\n",
        "\n",
        "The least formatted and therefor easiest-to-read flat-file format is the text file. However, a text file also treats all data as strings, so you often have to convert numeric data to into other forms. A comma-separated calue (CSV) file provides more formatting and more information, but it requires a little more effort to read. At the high end of flat-file formatting are custom data formats, such as an Excel file, which contains extensive formatting and could include multiple datasets in a single file.\n",
        "\n",
        "The following sections describe these three levels of flat-file datasets and show how to use them. These sections assume that the file structures the data in some way. For example, the CSV file uses commas to seperate data fields. An Excel file uses a complex method to seperate data fields and to provide a wealth of information about each field. You can work with unstructured data as well, but working with structured is easier because you know where each field begins and ends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FiGlqCDIZRa"
      },
      "source": [
        "### Reading from a text file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbwODv5aIZRb"
      },
      "source": [
        "Text files can use a variety of storage formats. However, a common format is to have a header line that documents the purpose of each field, followed by another line for each record in the file. The file separates the fields using tabs. \n",
        "\n",
        "Pandas library holds a set of parsers, code used to read individual bits of data and determine the purpose of each bit according to the format of the entire file. Using the right parser is essential, if you want to make any sence of file content. One example is using the 'read_tabel()' method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHjaFV7xIZRc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "table = pd.io.parsers.read_table('file_name.txt')\n",
        "print(table)\n",
        "# You can adjust how the parser interprets the input file but the default settings works well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgY_8NkmIZRc"
      },
      "source": [
        "### Reading CSV delimited format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaA3395MIZRd"
      },
      "source": [
        "The structure for a CSV file can look something like this:\n",
        "1. A header defines each of the fields\n",
        "2. Fields are separeted by commas\n",
        "3. Records are separeted by linefeeds\n",
        "4. Stings are enclosed in double quotes\n",
        "5. Integers and real numbers appear without double quotes\n",
        "    \n",
        "You can use an application such as Excel to create a CSV formatted presentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZP3wh77nIZRe",
        "outputId": "738a31b1-a193-4b3f-d9b6-e09f9967d855"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] File file_name.csv does not exist: 'file_name.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-84-aca342a14060>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'file_name.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'header'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# Could be 'age', 'sex' etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# X = file[['header']].values (turns output into list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File file_name.csv does not exist: 'file_name.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "file = pd.io.parsers.read_csv('file_name.csv')\n",
        "X = file[['header']] # Could be 'age', 'sex' etc.\n",
        "# X = file[['header']].values (turns output into list)\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9aBoGhNIZRf"
      },
      "source": [
        "### Reading Excel and other microsoft files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvKtwKvWIZRg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "xls = pd.ExcelFile(\"file_name.xls\")\n",
        "observations = xls.parse('Sheet1', index_col=None, # Generate an index\n",
        "                       na_values=['NA'])\n",
        "print(observations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BO0QgauIZRg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "observations = pd.read_excel(\"file_name.xls\", 'Sheet1', index_col=None, na_values=['NA'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43pnTlNIZRh"
      },
      "source": [
        "### Sending data in unstructured file form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APtCBHEBIZRh"
      },
      "source": [
        "Unstructured data files consist of a series of bits. The file does not on its own separate the bits and you can´t simply look into the file and see any structure. Therefor, unstructured data rely on the file user to know how to interpret the data. For example, a picture could consist of three 32-bit fields, which is up to you to find out. Here is an example of working with unstructured data in the form of a picture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgzlVskYIZRi"
      },
      "outputs": [],
      "source": [
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "example_file = (\"gädda.jpg\")\n",
        "image = imread(example_file, as_gray=True)\n",
        "plt.imshow(image, cmap=cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7u9l5WSIZRi"
      },
      "outputs": [],
      "source": [
        "print(\"data type: %s, shape: %s\" %\n",
        "     (type(image), image.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUCVKUVKIZRj"
      },
      "outputs": [],
      "source": [
        "image2 = image[50:1000,50:1000]\n",
        "plt.imshow(image2, cmap=cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO8q7lVKIZRj"
      },
      "outputs": [],
      "source": [
        "image3 = resize(image, (529//2,705//2), mode='symmetric')\n",
        "plt.imshow(image3, cmap=cm.gray)\n",
        "plt.show()\n",
        "print(\"data type: %s, shape: %s\" %\n",
        "     (type(image3), image3.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNbo8a3ZIZRj"
      },
      "outputs": [],
      "source": [
        "image_row = image3.flatten()\n",
        "print(\"data type: %s, shape: %s\" %\n",
        "     (type(image_row), image_row.shape))\n",
        "print(\"(264*352)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAjNCGR6IZRk"
      },
      "source": [
        "### Creating a connection to a sql database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvpSfH-UIZRk"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import create_engine\n",
        "engine = create_engine('sqlite:///:memory:')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfKunjY1IZRk"
      },
      "source": [
        "After you have accesse to an engine, you can use the engine to performe tasks specific to that DBMS (Database Management System). The output of a read method is always a DataFrame object that contains the requested data. To write data, you must create a DataFrame object or use an existing DataFrame object. You normally use these methods to perform most tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IAEj_FI5IZRl"
      },
      "outputs": [],
      "source": [
        "read_sql_tabel() \n",
        "# Reads data from a SQL table to a DatFrame object\n",
        "read_sql_query()\n",
        "# Reads data from a database using a SQL query to a DataFrame object\n",
        "read_sql()\n",
        "# Reads data from either a SQL table or query to a DataFrame object\n",
        "DataFrame.to_sql()\n",
        "# Writes the content of a DataFrame object to the specified tables in the database"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvZkr6d_IZRl"
      },
      "source": [
        "### NoSQL databases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-58xvCbIZRm"
      },
      "source": [
        "The process of using Not only SQL (NoSQL) databases:\n",
        "1. Import required database engine functionality\n",
        "2. Create a database engine\n",
        "3. Make any required queries using the database engine and the functionality supported by the DBMS.\n",
        "\n",
        "Discover more about working with datasets at sqlalchemy.org "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIswFN-0IZRm"
      },
      "outputs": [],
      "source": [
        "import pymongo\n",
        "import pandas as pd\n",
        "from pymongo import Connection\n",
        "connection = Connection()\n",
        "db = connection.database_name\n",
        "input_data = db.collection_name\n",
        "data = pd.DataFrame(list(input_data.find()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0VPbl9XIZRn"
      },
      "source": [
        "### XML"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "Q_cZv0egIZRn"
      },
      "source": [
        "XML is an hierarchical format that can become quite complex. This is an image example of how a .xml file can look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Jve2kUJFIZRn",
        "outputId": "49b9aa55-e420-4a7a-bd73-962a6fcb6243"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: 'XML.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m             \u001b[0mb64_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_repr_mimebundle_\u001b[1;34m(self, include, exclude)\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m             \u001b[0mmimetype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mimetype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malways_both\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1263\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m                 \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmimetype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m             raise FileNotFoundError(\n\u001b[1;32m-> 1275\u001b[1;33m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[0m\u001b[0;32m   1276\u001b[0m         \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: 'XML.jpg'"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "No such file or directory: 'XML.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1272\u001b[1;33m             \u001b[0mb64_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb2a_base64\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'str'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_png_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FMT_PNG\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_and_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_repr_jpeg_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\machinelearning\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36m_data_and_metadata\u001b[1;34m(self, always_both)\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m             raise FileNotFoundError(\n\u001b[1;32m-> 1275\u001b[1;33m                 \"No such file or directory: '%s'\" % (self.data))\n\u001b[0m\u001b[0;32m   1276\u001b[0m         \u001b[0mmd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: No such file or directory: 'XML.jpg'"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from IPython.display import Image\n",
        "Image(\"XML.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ib7EtjYxIZRo",
        "outputId": "f761831e-cf2d-484b-f30e-99aa46d8307b"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'XMLData.xml'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-86-da103b5bd197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mxml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjectify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'XMLData.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'XMLData.xml'"
          ]
        }
      ],
      "source": [
        "from lxml import objectify\n",
        "import pandas as pd\n",
        "\n",
        "xml = objectify.parse(open('XMLData.xml'))\n",
        "root = xml.getroot()\n",
        "\n",
        "df = pd.DataFrame(columns=['customer_id', 'first_name', 'last_name', 'email'])\n",
        "# For the example image\n",
        "\n",
        "for i in range(0,5):\n",
        "    obj = root.getchildren()[i].getchildren()\n",
        "    row = dict(zip(['customer_id', 'first_name', 'last_name', 'email'],\n",
        "                   [obj[0].text, obj[1].text,\n",
        "                    obj[2].text, obj[3].text]))\n",
        "    row_s = pd.Series(row)\n",
        "    row_s.name = i\n",
        "    df = df.append(row_s)\n",
        "    \n",
        "# Search for duplicates\n",
        "search = df.DataFrame.duplicated(df)\n",
        "print(df)\n",
        "print(search[search == True])\n",
        "\n",
        "# Remove duplicates\n",
        "print(df.drop_duplicates())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZV3MgoVIZRo"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdLBMRMfIZRp"
      },
      "source": [
        "1. Get the data\n",
        "2. Aggregate the data\n",
        "3. Create data subsets\n",
        "4. Clean the data\n",
        "5. Develop a single dataset by merging various datasets together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDV093dpIZRp"
      },
      "source": [
        "### Creating a data map and a data plan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2tkmSeCIZRp"
      },
      "source": [
        "You need to be aware of how your data looks statisticly. A data map is an overview of the dataset. You use it to spot potential problems in your data such as:\n",
        "\n",
        "1. Redundant variables\n",
        "2. Possible errors\n",
        "3. Missing values\n",
        "4. Variable transformations\n",
        "\n",
        "Cheching for these problems goes into a data plan, which is a list of tasks that you have to preform to ensure the integrity of your data. The following example shows a data map, A, with two datasets, B and C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umZZEZ0gIZRq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.width', 55)\n",
        "\n",
        "df = pd.DataFrame({'A': [0,0,0,0,0,1,1],\n",
        "                   'B': [1,2,3,5,4,2,5],\n",
        "                   'C': [5,3,4,1,1,2,3]})\n",
        "\n",
        "a_group_desc = df.groupby('A').describe()\n",
        "#print(a_group_desc)\n",
        "\n",
        "# stacked = a_group_desc.stack()\n",
        "# print(stacked)\n",
        "\n",
        "print(a_group_desc.loc[:,(slice(None),['count','mean']),])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoLfCAFzIZRq"
      },
      "source": [
        "### Creating categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_khkiuDIZRq"
      },
      "source": [
        "In data science, a categorical value is one that has a specific value from a limited selection of values. The number of values is usually fixed. Many developer will know categorical values by the moniker enumerations. Each of the potential values that a categoriacl variable can assume is called a level.\n",
        "\n",
        "Let´s say that you have an variable to express the color of a car. The computer interpret it as a numerical value, so normally when you print the color of a car, the value of the color is returned. If you use pandas.DataFrame, you can still use the symbolic value (blue, red, green), even tough the computer sees it as a numeric value. Sometimes you need to combine and rename these named values to create new symbols. \n",
        "\n",
        "Symbolic variables are just a convenient way of representing and storing qualitiative data. \n",
        "\n",
        "Some algorithms, such as trees and ensembles of three, can work directly with numerical values behind the symbols, while other require binary variables. This can be algorithms such as linear or logistic regression and SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oixR51DxIZRr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "car_colors = pd.Series(['Blue', 'Red', 'Green'],\n",
        "                      dtype='category')\n",
        "\n",
        "car_data = pd.Series(\n",
        "    pd.Categorical(\n",
        "        ['Yellow', 'Green', 'Red', 'Blue', 'Purple'],\n",
        "                    categories=car_colors, ordered=False))\n",
        "\n",
        "# Find missing values\n",
        "find_entries = pd.isnull(car_data)\n",
        "\n",
        "print(car_colors)\n",
        "print()\n",
        "print(car_data)\n",
        "print()\n",
        "print(find_entries[find_entries == True])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4boD-ImIZRr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "car_colors = pd.Series(['Blue', 'Red', 'Green'],\n",
        "                      dtype='category')\n",
        "\n",
        "car_data = pd.Series(\n",
        "    pd.Categorical(\n",
        "        ['Yellow', 'Green', 'Red', 'Blue', 'Purple'],\n",
        "                    categories=car_colors, ordered=False))\n",
        "\n",
        "car_colors.cat.categories = ['Purple', 'Yellow', 'Mauve']\n",
        "car_data.cat.categories = car_colors\n",
        "\n",
        "print(car_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zSfFhPKIZRs"
      },
      "source": [
        "### Combining levels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHu4IrFRIZRs"
      },
      "source": [
        "A particular categorical level might be too small to offer significant data to analysis. Perhaps there are only a few of the values, which may not be enough to create a statistical difference. In this case, combining several small categories might offer better analysis results. The following example shows how to combine categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFli9EcuIZRt",
        "outputId": "e6465569-4f24-4fdb-841f-8099ea3412f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    Red\n",
            "4    Red\n",
            "dtype: category\n",
            "Categories (4, object): ['Blue', 'Red', 'Green', 'Blue_Red']\n",
            "\n",
            "0    Blue_Red\n",
            "1       Green\n",
            "2    Blue_Red\n",
            "3       Green\n",
            "4    Blue_Red\n",
            "5       Green\n",
            "dtype: category\n",
            "Categories (2, object): ['Green', 'Blue_Red']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "car_colors = pd.Series(['Blue', 'Red', 'Green'],\n",
        "    dtype='category')\n",
        "car_data = pd.Series(\n",
        "    pd.Categorical(\n",
        "    ['Blue','Green','Red','Green','Red','Green'],\n",
        "    categories=car_colors, ordered=False))\n",
        "\n",
        "car_data = car_data.cat.set_categories(\n",
        "    ['Blue','Red','Green','Blue_Red'])\n",
        "print(car_data.loc[car_data.isin(['Red'])])\n",
        "car_data.loc[car_data.isin(['Red'])] = 'Blue_Red'\n",
        "car_data.loc[car_data.isin(['Blue'])] = 'Blue_Red'\n",
        "car_data = car_data.cat.set_categories(\n",
        "    ['Green', 'Blue_Red'])\n",
        "\n",
        "print()\n",
        "print(car_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4dYcuDnIZRt"
      },
      "source": [
        "### Date and time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlWF_0g-IZRu"
      },
      "source": [
        "Keep in mind that Excel users can choose to start dates in 1900 or 1904 and the numeric encoding for each is diferent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKFbW9jtIZRv"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "\n",
        "now = dt.datetime.now()\n",
        "\n",
        "print(str(now))\n",
        "print(now.strftime('%a, %d, %B, %Y'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CPEu2w7IZRv"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "\n",
        "now = dt.datetime.now()\n",
        "timevalue = now + dt.timedelta(hours=2)\n",
        "\n",
        "print(now.strftime('%a, %d, %B, %Y'))\n",
        "print(now.strftime('%H:%M:%S'))\n",
        "print(timevalue.strftime('%H:%M:%S'))\n",
        "\n",
        "if now < timevalue:\n",
        "    print(timevalue - now)\n",
        "else:\n",
        "    print(now - timevalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ugoz5roIZRw"
      },
      "source": [
        "### Dealing with missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2dN3mgwIZRw",
        "outputId": "abc33cb0-1690-4bf7-de1c-f1d713085d2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    False\n",
            "1    False\n",
            "2    False\n",
            "3     True\n",
            "4    False\n",
            "5    False\n",
            "6     True\n",
            "dtype: bool\n",
            "\n",
            "3   NaN\n",
            "6   NaN\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Finding the missing data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "s = pd.Series([1, 2, 3, np.NaN, 5, 6, None])\n",
        "\n",
        "print(s.isnull())\n",
        "\n",
        "print()\n",
        "print(s[s.isnull()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NgDvw8rIZRw"
      },
      "source": [
        "Handle the missing data by ignoring, filling or removing (drop) it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwZ6MGSfIZRx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "s = pd.Series([1, 2, 3, np.NaN, 5, 6, None])\n",
        "\n",
        "print(s.fillna(int(s.mean())))\n",
        "# Fill in the missing values using the mean (medelvärde)\n",
        "print()\n",
        "print(s.dropna())\n",
        "# Drop the missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lp8iXp7lIZRx"
      },
      "source": [
        "Working with series is straightforward because the dataset is simple. However, when working with a dataframe it is not as simple. You still have the option of dropping the entire row but when a column is sparsely populated, you might want to drop the column instead. Filling in the data also becomes more complicated because you must consider the dataset as a whole, in addition to the needs of the individual feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe59mFZWIZRy",
        "outputId": "96033b17-69d2-4207-d2b2-9717fd82d97a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    1.0\n",
            "1    2.0\n",
            "2    3.0\n",
            "3    4.0\n",
            "4    5.0\n",
            "5    6.0\n",
            "6    7.0\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Imputing missing data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer as imputer\n",
        "\n",
        "s = [[1, 2, 3, np.NaN, 5, 6, None]]\n",
        "\n",
        "imp = imputer(missing_values=np.nan, strategy='mean')\n",
        "# mean, median, most_frequent\n",
        "\n",
        "imp.fit([[1, 2, 3, 4, 5, 6, 7]])\n",
        "\n",
        "x = pd.Series(imp.transform(s).tolist()[0])\n",
        "\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYSJ0wSzIZRy"
      },
      "source": [
        "### Slicing rows and columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JNZb7ozIZRy"
      },
      "source": [
        "Slicing a row from a row of 2-D or 3-D data allows you to focus on specific data. In some cases, you might associate rows with cases in a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6ev1LBTIZR6",
        "outputId": "c7f1e3e4-09a1-4bb3-9c83-2bf5fb63f83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sliced row: \n",
            " [[11 12 13]\n",
            " [14 15 16]\n",
            " [17 18 19]]\n",
            "\n",
            "Sliced column: \n",
            " [[ 4  5  6]\n",
            " [14 15 16]\n",
            " [24 25 26]]\n"
          ]
        }
      ],
      "source": [
        "# An example of a 3-D array\n",
        "x = np.array([[[1,2,3], [4,5,6], [7,8,9]],\n",
        "             [[11,12,13], [14,15,16], [17,18,19]],\n",
        "             [[21,22,23], [24,25,26], [27,28,29]]])\n",
        "\n",
        "\n",
        "# Slice row\n",
        "print(f\"Sliced row: \\n {x[1]}\")\n",
        "\n",
        "# Slice column\n",
        "print(f\"\\nSliced column: \\n {x[:,1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRle-pyLIZR6"
      },
      "source": [
        "### Dicing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOkzS3BYIZR7"
      },
      "source": [
        "Dicing means preforming both a slicing of a row and column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijhIgu0DIZR7"
      },
      "outputs": [],
      "source": [
        "x = np.array([[[1,2,3], [4,5,6], [7,8,9],],\n",
        "             [[11,12,13], [14,15,16], [17,18,19],],\n",
        "             [[21,22,23], [24,25,26], [27,28,29]]])\n",
        "\n",
        "# Row and column\n",
        "print(x[1,2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrI0g2rZIZR7"
      },
      "source": [
        "### Concatenating and transforming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A_dsl--IZR8"
      },
      "source": [
        "Combine diferent datasets with diferent formats into a single dataset. You also need to keep in mind that the same data can be in diferent forms, for example, age can be both integer or string. For the fields to work together, they must appear as the same type of information. \n",
        "\n",
        "You often find a need to combine datasets in various ways or even to add new information for the sake of analysis purposes. The result is a combined dataset that includes either new cases or variables. The following example shows techniques for preforming both tasks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBYKbsGPIZR8"
      },
      "source": [
        "### Combining dataframes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KbVPHPRIZR8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'A': [2,3,1],\n",
        "                   'B': [1,2,3],\n",
        "                   'C': [5,3,4]})\n",
        "\n",
        "df1 = pd.DataFrame({'A': [4],\n",
        "                    'B': [4],\n",
        "                    'C': [4]})\n",
        "\n",
        "df = df.append(df1) # concat() \n",
        "df = df.reset_index(drop=True)\n",
        "print(df)\n",
        "\n",
        "df.loc[df.last_valid_index() + 1] = [5,5,5]\n",
        "print()\n",
        "print(df)\n",
        "\n",
        "df2 = pd.DataFrame({'D': [1, 2, 3, 4, 5]})\n",
        "\n",
        "df = pd.DataFrame.join(df, df2)\n",
        "print()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IptZRFI2IZR9"
      },
      "source": [
        "### Removing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7mtZmI5AIZR9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'A': [2,3,1],\n",
        "                   'B': [1,2,3],\n",
        "                   'C': [5,3,4]})\n",
        "\n",
        "df = df.drop(df.index[1])\n",
        "print(df)\n",
        "\n",
        "df = df.drop('B', axis=1)\n",
        "df = df.drop(2, axis=0)\n",
        "print()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OlOH-x0IZR-"
      },
      "source": [
        "### Sorting and shuffling data order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1m5eu0hIZR-"
      },
      "source": [
        "You sort and shuffle in order to manage data order. This might mess up your results for the purpose of analysis but can be useful for presentation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oglpa9mFIZR_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.DataFrame({'A': [1,5,2,3,5,6,7],\n",
        "                'B': [5,3,6,1,5,8,9],\n",
        "                'C': [7,2,4,6,8,3,3]})\n",
        "\n",
        "# Sort\n",
        "df = df.sort_values(by=['A','C'], ascending=[True, True])\n",
        "df = df.reset_index(drop=False)\n",
        "print(df)\n",
        "\n",
        "\n",
        "# Shuffle\n",
        "index = df.index.tolist()\n",
        "np.random.shuffle(index)\n",
        "df = df.loc[df.index[index]]\n",
        "df = df.reset_index(drop=True)\n",
        "print()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO1oUe4mIZR_"
      },
      "source": [
        "### Aggregating data at any level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66Nhk-BkIZSA"
      },
      "source": [
        "Aggregation is the process of combining or grouping data together into a set, bag or list. The data may or may not be alike. However, in most cases, an aggregation function combines several rows together statistically, using algorithms such as average, count, maximum, median, minimum, mode or sum. There are several reasons to aggreagte data:\n",
        "\n",
        "1. Make it easier to analyse\n",
        "2. Reduce the ability of anyone to deduce the data of an individual from the dataset for privace and other reasons. \n",
        "3. Create a combined data element from one data source that matches a combined data element in another source. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpvVDtRsIZSA",
        "outputId": "2114cc83-be34-4478-d63d-2966715e34b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Map  Values  S    M    V\n",
            "0    0       1  6  2.0  1.0\n",
            "1    0       2  6  2.0  1.0\n",
            "2    0       3  6  2.0  1.0\n",
            "3    1       5  9  4.5  0.5\n",
            "4    1       4  9  4.5  0.5\n",
            "5    2       2  7  3.5  4.5\n",
            "6    2       5  7  3.5  4.5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Map': [0,0,0,1,1,2,2,],\n",
        "                   'Values': [1,2,3,5,4,2,5]})\n",
        "\n",
        "df['S'] = df.groupby('Map')['Values'].transform(np.sum)\n",
        "df['M'] = df.groupby('Map')['Values'].transform(np.mean)\n",
        "df['V'] = df.groupby('Map')['Values'].transform(np.var)\n",
        "\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "GT3AtU1hIZQ3",
        "UZ79Bc2GIZQ8",
        "IjUkg2q9IZRA",
        "SxE4mWX1IZRC",
        "GDoIpd4QIZRE",
        "0aOSSofLIZRX",
        "-FiGlqCDIZRa",
        "YgY_8NkmIZRc",
        "t9aBoGhNIZRf",
        "y43pnTlNIZRh",
        "xAjNCGR6IZRk",
        "IvZkr6d_IZRl",
        "q0VPbl9XIZRn",
        "9ZV3MgoVIZRo",
        "pDV093dpIZRp",
        "SoLfCAFzIZRq",
        "1zSfFhPKIZRs",
        "S4dYcuDnIZRt",
        "7ugoz5roIZRw",
        "wYSJ0wSzIZRy",
        "NRle-pyLIZR6",
        "YrI0g2rZIZR7",
        "bBYKbsGPIZR8",
        "IptZRFI2IZR9",
        "6OlOH-x0IZR-",
        "MO1oUe4mIZR_"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}